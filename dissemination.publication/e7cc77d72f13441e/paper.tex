\documentclass[sigplan]{acmart}

\usepackage{booktabs} % For formal tables


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


\newenvironment{packed_itemize}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

% DOI
\acmDOI{10.1145/3229762.3229763}

% ISBN
% \acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[ReQuEST at ASPLOS'18]{1st ACM Reproducible Quality-Efficient Systems Tournament on Co-designing Pareto-efficient Deep Learning}{March 2018}{Williamsburg, VA, USA}
\acmYear{2018}
\copyrightyear{2018}

%\acmPrice{15.00}

%\acmBadgeL[http://ctuning.org/ae/ppopp2016.html]{ae-logo}
%\acmBadgeR[http://ctuning.org/ae/ppopp2016.html]{ae-logo} 

\begin{document}

\title{Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe \\}

\author{Jiong Gong, Jiong Gong, Haihao Shen, Guoming Zhang, Xiaoli Liu, Shane Li, Ge Jin, Niharika Maheshwari, Evarist Fomenko, Eden Segal}
\affiliation{%
  \institution{Intel Corporation}
}
\email{jiong.gong, haihao.shen, guoming.zhang, xiaoli.liu, li.shane, ge.jin, niharika.maheshwari, evarist.m.fomenko, eden.segal@intel.com}

\renewcommand{\shortauthors}{}
\renewcommand{\shorttitle}{}

\begin{abstract}
High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents the efficient inference techniques of IntelCaffe, the first Intel\textsuperscript{\textregistered} optimized deep learning framework that supports efficient 8-bit low precision inference and model optimization techniques of convolutional neural networks on Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} Scalable Processors. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We show that the inference throughput and latency with ResNet-50, Inception-v3 and SSD are improved by 1.38X-2.9X and 1.35X-3X respectively with neglectable accuracy loss from IntelCaffe FP32 baseline and by 56X-75X and 26X-37X from BVLC Caffe. All these techniques have been open-sourced on IntelCaffe GitHub\footnote{https://github.com/intel/caffe}, and the artifact is provided to reproduce the result on Amazon AWS Cloud.
\end{abstract}

\keywords{Intel Caffe, Convolutional Neural Network, Deep Learning, Model Optimization}

\maketitle

\input{paper_text}

\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}

\newpage

\onecolumn

\appendix
\section{Artifact Appendix}

Submission and reviewing methodology: \\
{\em http://cTuning.org/ae/submission-20171101.html}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Abstract}

This Artifact Appendix describes experimental workflow,
artifacts and results from this paper evaluated 
during the 1st reproducible ReQuEST tournament at the ACM ASPLOS'18:

\begin{packed_itemize}
  \item {\bf Latest CK workflow:} \url{https://github.com/ctuning/ck-request-asplos18-caffe-intel}
  \item {\bf CK results:} \url{https://github.com/ctuning/ck-request-asplos18-results-caffe-intel}
  \item {\bf Artifact DOI:} \url{https://doi.org/10.1145/3229769}
  \item {\bf ReQuEST submission and reviewing guidelines:} \url{http://cknowledge.org/request-cfp-asplos2018.html} (\cite{request-asplos18})
  \item {\bf ReQuEST goals:} \cite{cm:29db2248aba45e59:0c7348dfbadd5b95}
  \item {\bf ReQuEST workflows:} \url{https://github.com/ctuning/ck-request-asplos18-results}
  \item {\bf ReQuEST scoreboard:} \url{http://cKnowledge.org/request-results}
\end{packed_itemize}

Hardware and software configurations and execution steps are
provided to reproduce the inference throughput and latency
result mentioned in this paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Artifact check-list}

Details: \url{http://cTuning.org/ae/submission_extra.html}

\begin{itemize}
  \item {\bf Algorithm}: image classification
  \item {\bf Compilation: } Intel C++ Compiler 17.0.5 20170817
  \item {\bf Binary: } will be compiled on a target platform (caffe)
  \item {\bf Data set: } Random data for throughput and latency. ImageNet 2012 validation (50,000 images) and PASCAL VOC2007 for accuracy.
  \item {\bf Run-time environment: }
  KMP\_HW\_SUBSET=1T \\
  KMP\_AFFINITY=granularity=fine,compact \\
  OMP\_NUM\_THREADS=18
  \item {\bf Hardware: } single socket (18 cores) on AWS c5.18xlarge.
  \item {\bf Execution: } automated via CK command line (python benchmark.py -mode \newline accuracy[fps$|$throughput])
  \item {\bf Metrics: } \\ Throughput: images per second. \\ Latency: milli-second. \\ Accuracy: \% top-1/top-5/mAP.
  \item {\bf Output: } classification result; execution time; accuracy
  \item {\bf Experiments: } We use batch size 64, 64, and 32 to measure the throughput for ResNet-50, Inception-V3, and SSD respectively. We use batch size 1 to measure the latency.
  \item {\bf How much disk space required (approximately)?:} 
  \item {\bf How much time is needed to prepare workflow (approximately)?:} 
  \item {\bf How much time is needed to complete experiments (approximately)?:}
  \item {\bf Publicly available?:} Yes
  \item {\bf Code license?:} 
  \item {\bf Collective Knowledge workflow framework used?:} Yes
  \item {\bf CK workflow URL:} \url{https://github.com/ctuning/ck-request-asplos18-caffe-intel}
  \item {\bf CK results URL:} \url{https://github.com/ctuning/ck-request-asplos18-results-caffe-intel}
  \item {\bf Original artifact URL:} \url{https://github.com/intel/caffe/wiki/ReQuEST-Artifact-Installation-Guide}

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description}

\subsubsection{How delivered}
AWS Public Deep Learning Image containing pre-built BVLC Caffe, IntelCaffe GitHub.
\subsubsection{Hardware dependencies}
Amazon AWS c5.18xlarge instance.
\subsubsection{Software dependencies}
IntelCaffe (branch: request\_artifact), BVLC Caffe, Intel\textsuperscript{\textregistered} C++ Compiler
\subsubsection{Data sets}
Fake dataset, ImageNet-1k and PASCAL VOC2007
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Installation}
https://github.com/intel/caffe/wiki/ReQuEST-Artifact-Installation-Guide

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment workflow}
On throughput measurement, we use batch size 64, 64, and 32 for ResNet-50, Inception-V3, and SSD, respectively. We measure the latency with the single batch on one socket. On accuracy, the auxiliary script would first calibrate the FP32 models of ResNet-50, Inception-V3 and SSD on the ImageNet-1k and PASCAL VOC2007 dataset, generating 8-bit low precision prototxt files. Then these files can be executed on the same datasets to get the test accuracy.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation and expected result}
You will get the similar performance data within 5\% range for both throughput and latency. Since AWS instances are virtualized, there might be larger variances than bare metal machines. I recommend the evaluators to collect the performance data three times to pick the stable data points. On accuracy, you will get equivalent accuracy result.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notes}
For SSD, we added DetectOutput layer in order to calculate the mAP.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Unified installation and evaluation for the ReQuEST scoreboard using Collective Knowledge framework}

\subsubsection{Minimal CK installation}

The minimal installation requires:

\begin{packed_itemize}
 \item Python 2.7 or 3.3+ (limitation is mainly due to unitests)
 \item Git command line client.
\end{packed_itemize}

You can install CK in your local user space as follows:

\begin{verbatim}
$ git clone http://github.com/ctuning/ck
$ export PATH=$PWD/ck/bin:$PATH
$ export PYTHONPATH=$PWD/ck:$PYTHONPATH
\end{verbatim}

You can also install CK via PIP with sudo to avoid setting up environment variables yourself:

\begin{verbatim}
$ sudo pip install ck
\end{verbatim}

\subsubsection{Install Intel Caffe from the ReQuEST artifact branch}

\begin{verbatim}
$ ck pull repo:ck-request-asplos18-caffe-intel
$ ck install package:lib-caffe-intel-request-cpu
\end{verbatim}

\subsubsection{Install global software dependencies for Caffe (Ubuntu)}

Please, follow installation guide from the ck-caffe repository: \url{https://github.com/dividiti/ck-caffe}:

\textbf{Installing general dependencies}

\begin{verbatim}
$ sudo apt install coreutils \
                   build-essential \
                   make \
                   cmake \
                   wget \
                   git \
                   python \
                   python-pip
\end{verbatim}

\textbf{Installing essential Caffe dependencies}
\begin{verbatim}
$ sudo apt install libleveldb-dev \
                   libsnappy-dev \
                   gfortran
\end{verbatim}

\textbf{Installing optional Caffe dependencies}
CK can automatically build the following dependencies from source using versions that should work well together. Installing via `apt`, however, is somewhat faster.

\begin{verbatim}
$ sudo apt install libboost-all-dev \
                   libgflags-dev \
                   libgoogle-glog-dev \
                   libhdf5-serial-dev \
                   liblmdb-dev \
                   libprotobuf-dev \
                   protobuf-compiler \
                   libopencv-dev
$ sudo pip install protobuf
\end{verbatim}

\subsubsection{Install reference Caffe CPU version}

You can install refernece Caffe CPU version using the following CK package:
\begin{verbatim}
$ ck install package:lib-caffe-bvlc-master-cpu-universal
\end{verbatim}

You can use it to prepare ImageNet validation datasets

\subsubsection{Install ImageNet validation datasets}

**NB:** If you already have the ImageNet validation dataset downloaded, e.g. in
`/datasets/ilsvrc2012\_val/`, you can register it with CK as follows:

\begin{verbatim}
$ ck detect soft:dataset.imagenet.val --full_path=/datasets/ilsvrc2012_val/ILSVRC2012_val_00000001.JPEG
\end{verbatim}

\textbf{Reduced (500 images)}
\begin{verbatim}
$ ck install package:imagenet-2012-val-min
\end{verbatim}

\textbf{Full (50,000 images)}
\begin{verbatim}
$ ck install package:imagenet-2012-val
\end{verbatim}

\textbf{Install Caffe models and resize ImageNet dataset}

**NB:** If you already have the ImageNet validation dataset resized as an LMDB file, 
e.g. in `/datasets/dataset-imagenet-ilsvrc2012-val-lmdb-dataset.imagenet.val-ilsvrc2012\_val\_full-resize-320/data/data.mdb`, 
you can register it with CK as follows:

\begin{verbatim}
$ ck detect soft:dataset.imagenet.val.lmdb --full_path=/datasets/dataset-imagenet-ilsvrc2012-val-lmdb-
    dataset.imagenet.val-ilsvrc2012_val_full-resize-320/data/data.mdb
\end{verbatim}

\textbf{ResNet50}

**NB:** ResNet50 uses an ImageNet mean file of resolution `224x224`, so the inputs must match that.

\begin{verbatim}
$ ck install ck-caffe:package:imagenet-2012-val-lmdb-224
$ ck install ck-caffe:package:caffemodel-resnet50
$ ck install ck-request-asplos18-caffe-intel:package:caffemodel-resnet50-intel-i8
\end{verbatim}

\textbf{Inception-v3}

**NB:** Inception-v3 uses an ImageNet mean file of resolution `320x320`, so the inputs must match that.

\begin{verbatim}
$ ck install ck-caffe:package:imagenet-2012-val-lmdb-320
$ ck install ck-caffe:package:caffemodel-inception-v3
$ ck install ck-request-asplos18-caffe-intel:package:caffemodel-inception-v3-intel-i8
\end{verbatim}

\textbf{SSD}

\begin{verbatim}
$ ck install package:caffemodel-ssd-voc-300
\end{verbatim}

\subsubsection{Detect Intel compilers and install Intel Caffe}

You must have Intel compilers installed on your system, for example in /opt/intel.
In such case you can register Intel compilers in the CK as follows:
\begin{verbatim}
$ ck detect soft:compiler.icc --search_dirs=/opt/intel

$ ck show env --tags=compiler
\end{verbatim}

You can now install Intel Caffe as follows (select detect Intel compiler if asked by CK):
\begin{verbatim}
$ ck install package:lib-caffe-intel-request-cpu
\end{verbatim}

\subsection{Usage instructions}

\subsubsection{Measure accuracy}
\begin{verbatim}
$ ck run program:caffe --cmd_key=test_cpu
\end{verbatim}

Results:
\begin{packed_itemize}
 \item \url{https://github.com/ctuning/ck-request-asplos18-caffe-intel/issues/7#issuecomment-374265425}
 \item \url{https://github.com/ctuning/ck-request-asplos18-caffe-intel/issues/9#issuecomment-374268187}
\end{packed_itemize}

\subsubsection{Measure latency}
\begin{verbatim}
$ ck run program:caffe --cmd_key=time_cpu --env.CK_CAFFE_BATCH_SIZE=1
\end{verbatim}

\subsubsection{Measure throughput}
\begin{verbatim}
$ ck run program:caffe --cmd_key=time_cpu --env.CK_CAFFE_BATCH_SIZE=64
\end{verbatim}

\subsubsection{Explore performance}

Explore how the execution time is affected by changing:
\begin{packed_itemize}
 \item [`nt`] the number of OpenMP threads (e.g. from 1 to 20 on a 10-core machine with hyperthreading);
 \item [`bs`] the batch size (e.g. from 1 to 64).
\end{packed_itemize}

**NB:** You may want to change the `bs` and `nt` space exploration parameters, as well as
`platform\_tags` in the `benchmarking.py` script before launching it as follows:

\begin{verbatim}
$ python `ck find script:explore-batch-size-openmp-threads`/benchmarking.py
\end{verbatim}

\subsection{Unify output and add extra dimensions}

Scripts to unify all experiments and add extra dimensions in ReQuEST format for further comparison and visualization are available in the following entry:
\begin{verbatim}
$ cd `ck find ck-request-asplos18-caffe-intel:script:explore-batch-size-openmp-threads`
\end{verbatim}

\begin{packed_itemize}
 \item benchmark-merge-performance-with-accuracy.py - merges performance entries with accuracy
 \item benchmark-add-dimensions-*.py - adds extra dimensions for different platforms
\end{packed_itemize}

CPU price is taken from \url{https://ark.intel.com/products/81705/Intel-Xeon-Processor-E5-2650-v3-25M-Cache-2\_30-GHz}.

All updated experimental results are then moved to the ck-request-asplos18-results-caffe-intel repository: \url{https://github.com/ctuning/ck-request-asplos18-results-caffe-intel}.
The best configurations are also moved to the ck-request-asplos18-results repo: \url{https://github.com/ctuning/ck-request-asplos18-results}.

\end{document}

